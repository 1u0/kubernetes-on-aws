#cloud-config
coreos:
  update:
    reboot-strategy: "off"
  flannel:
    interface: $private_ipv4
    etcd_endpoints: {{ .ETCDEndpoints }}
  units:
    - name: docker.service
      drop-ins:
        - name: 40-flannel.conf
          content: |
            [Unit]
            Requires=flanneld.service
            After=flanneld.service
            [Service]
            EnvironmentFile=/etc/kubernetes/cni/docker_opts_cni.env

    - name: flanneld.service
      drop-ins:
        - name: 10-etcd.conf
          content: |
            [Service]
            ExecStartPre=/usr/bin/etcdctl \
            --endpoint={{ .ETCDEndpoints }} set /coreos.com/network/config \
            '{ "Network": "{{.PodCIDR}}", "Backend": {"Type": "vxlan"}}'
    - name: kubelet.service
      command: start
      runtime: true
      content: |
        [Service]
        Environment=KUBELET_VERSION={{.K8sVer}}
        Environment=KUBELET_ACI=docker://{{.HyperkubeImageRepo}}
        Environment="RKT_OPTS=--volume dns,kind=host,source=/etc/resolv.conf \
        --mount volume=dns,target=/etc/resolv.conf \
        --volume rkt,kind=host,source=/opt/bin/host-rkt \
        --mount volume=rkt,target=/usr/bin/rkt \
        --volume var-lib-rkt,kind=host,source=/var/lib/rkt \
        --mount volume=var-lib-rkt,target=/var/lib/rkt \
        --volume stage,kind=host,source=/tmp \
        --mount volume=stage,target=/tmp \
        --insecure-options=image \
        --volume var-log,kind=host,source=/var/log \
        --mount volume=var-log,target=/var/log"
        ExecStartPre=/usr/bin/mkdir -p /var/log/containers
        ExecStart=/usr/lib/coreos/kubelet-wrapper \
        --cni-conf-dir=/etc/kubernetes/cni/net.d \
        --network-plugin={{.K8sNetworkPlugin}} \
        --container-runtime={{.ContainerRuntime}} \
        --rkt-path=/usr/bin/rkt \
        --lock-file=/var/run/lock/kubelet.lock \
        --exit-on-lock-contention \
        --register-schedulable=false \
        --node-ip=$public_ipv4 \
        --allow-privileged \
        --node-labels=master=true \
        --pod-manifest-path=/etc/kubernetes/manifests \
        --cluster_dns={{.DNSServiceIP}} \
        --cluster_domain=cluster.local \
        --kubeconfig=/etc/kubernetes/kubeconfig \
        --require-kubeconfig
        Restart=always
        RestartSec=10

        [Install]
        WantedBy=multi-user.target

    - name: decrypt-tls-assets.service
      enable: true
      content: |
        [Unit]
        Description=decrypt kubelet tls assets using amazon kms
        Before=kubelet.service

        [Service]
        Type=oneshot
        RemainAfterExit=yes
        ExecStart=/opt/bin/decrypt-tls-assets

        [Install]
        RequiredBy=kubelet.service

    - name: install-kube-system.service
      command: start
      runtime: true
      content: |
        [Service]
        Type=simple
        StartLimitInterval=0
        Restart=on-failure
        ExecStartPre=systemctl is-active docker.service
        ExecStartPre=systemctl is-active kubelet.service
        ExecStartPre=/usr/bin/curl http://127.0.0.1:8080/version
        ExecStart=/opt/bin/install-kube-system

    - name: copy-cni-plugins.service
      command: start
      runtime: true
      content: |
        [Unit]
        Description=copy cni plugins from hyperkube image
        Before=kubelet.service

        [Service]
        Type=oneshot
        RemainAfterExit=yes
        ExecStart=/usr/bin/rkt --insecure-options=image fetch \
        docker://{{.HyperkubeImageRepo}}:{{.K8sVer}}
        ExecStart=/usr/bin/rkt image extract {{.HyperkubeImageRepo}}:{{.K8sVer}} \
        --rootfs-only --overwrite /hyperkube-copy
        ExecStart=/usr/bin/mv /hyperkube-copy/opt/cni /opt
        ExecStart=/usr/bin/rm -rf /hyperkube-copy

write_files:
  - path: /etc/kubernetes/kubeconfig
    permissions: "0644"
    owner: core
    content: |
      apiVersion: v1
      kind: Config
      clusters:
      - name: local
        cluster:
          server: {{.SecureAPIServers}}
          certificate-authority: /etc/kubernetes/ssl/ca.pem
      users:
      - name: kubelet
        user:
          client-certificate: /etc/kubernetes/ssl/worker.pem
          client-key: /etc/kubernetes/ssl/worker-key.pem
      contexts:
      - context:
          cluster: local
          user: kubelet

  - path: /opt/bin/install-kube-system
    permissions: 0700
    owner: root:root
    content: |
      #!/bin/bash -e
      /usr/bin/curl  -H "Content-Type: application/yaml" -XPOST \
      -d"$(cat /srv/kubernetes/manifests/kube-dns-rc.yaml)" \
      "http://127.0.0.1:8080/api/v1/namespaces/kube-system/replicationcontrollers"

      /usr/bin/curl  -H "Content-Type: application/yaml" -XPOST \
      -d"$(cat /srv/kubernetes/manifests/kube-dashboard-rc.yaml)" \
      "http://127.0.0.1:8080/api/v1/namespaces/kube-system/replicationcontrollers"

      /usr/bin/curl  -H "Content-Type: application/yaml" -XPOST \
      -d"$(cat /srv/kubernetes/manifests/heapster-de.yaml)" \
      "http://127.0.0.1:8080/apis/extensions/v1beta1/namespaces/kube-system/deployments"

      /usr/bin/curl  -H "Content-Type: application/yaml" -XPOST \
      -d"$(cat /srv/kubernetes/manifests/kube-proxy.yaml)" \
      "http://127.0.0.1:8080/apis/extensions/v1beta1/namespaces/kube-system/daemonsets"

      /usr/bin/curl  -H "Content-Type: application/yaml" -XPOST \
      -d"$(cat /srv/kubernetes/manifests/kubelet.yaml)" \
      "http://127.0.0.1:8080/apis/extensions/v1beta1/namespaces/kube-system/daemonsets"

      for manifest in {kube-dns,heapster,kube-dashboard}-svc.yaml;do
          /usr/bin/curl  -H "Content-Type: application/yaml" -XPOST \
          -d"$(cat /srv/kubernetes/manifests/$manifest)" \
          "http://127.0.0.1:8080/api/v1/namespaces/kube-system/services"
      done

  - path: /etc/kubernetes/cni/docker_opts_cni.env
    content: |
      DOCKER_OPT_BIP=""
      DOCKER_OPT_IPMASQ=""

  - path: /opt/bin/host-rkt
    permissions: 0755
    owner: root:root
    content: |
      #!/bin/sh
      # This is bind mounted into the kubelet rootfs and all rkt shell-outs go
      # through this rkt wrapper. It essentially enters the host mount namespace
      # (which it is already in) only for the purpose of breaking out of the chroot
      # before calling rkt. It makes things like rkt gc work and avoids bind mounting
      # in certain rkt filesystem dependancies into the kubelet rootfs. This can
      # eventually be obviated when the write-api stuff gets upstream and rkt gc is
      # through the api-server. Related issue:
      # https://github.com/coreos/rkt/issues/2878
      exec nsenter -m -u -i -n -p -t 1 -- /usr/bin/rkt "$@"

  - path: /opt/bin/decrypt-tls-assets
    owner: root:root
    permissions: 0700
    content: |
      #!/bin/bash -e

      for encKey in $(find /etc/kubernetes/ssl/*.pem.enc);do
        cp $encKey ${encKey%.enc}
      done

  - path: /etc/kubernetes/manifests/kube-apiserver.yaml
    content: |
      apiVersion: v1
      kind: Pod
      metadata:
        name: kube-apiserver
        namespace: kube-system
      spec:
        hostNetwork: true
        containers:
        - name: kube-apiserver
          image: {{.HyperkubeImageRepo}}:{{.K8sVer}}
          command:
          - /hyperkube
          - apiserver
          - --bind-address=0.0.0.0
          - --etcd-servers={{ .ETCDEndpoints }}
          - --allow-privileged=true
          - --service-cluster-ip-range={{.ServiceCIDR}}
          - --secure-port=443
          - --advertise-address=$private_ipv4
          - --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,ResourceQuota
          - --tls-cert-file=/etc/kubernetes/ssl/apiserver.pem
          - --tls-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem
          - --client-ca-file=/etc/kubernetes/ssl/ca.pem
          - --service-account-key-file=/etc/kubernetes/ssl/apiserver-key.pem
          - --runtime-config=extensions/v1beta1/networkpolicies=true
          livenessProbe:
            httpGet:
              host: 127.0.0.1
              port: 8080
              path: /healthz
            initialDelaySeconds: 15
            timeoutSeconds: 15
          ports:
          - containerPort: 443
            hostPort: 443
            name: https
          - containerPort: 8080
            hostPort: 8080
            name: local
          volumeMounts:
          - mountPath: /etc/kubernetes/ssl
            name: ssl-certs-kubernetes
            readOnly: true
          - mountPath: /etc/ssl/certs
            name: ssl-certs-host
            readOnly: true
        volumes:
        - hostPath:
            path: /etc/kubernetes/ssl
          name: ssl-certs-kubernetes
        - hostPath:
            path: /usr/share/ca-certificates
          name: ssl-certs-host

  - path: /etc/kubernetes/manifests/kube-controller-manager.yaml
    content: |
      apiVersion: v1
      kind: Pod
      metadata:
        name: kube-controller-manager
        namespace: kube-system
      spec:
        containers:
        - name: kube-controller-manager
          image: {{.HyperkubeImageRepo}}:{{.K8sVer}}
          command:
          - /hyperkube
          - controller-manager
          - --master=http://127.0.0.1:8080
          - --leader-elect=true
          - --service-account-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem
          - --root-ca-file=/etc/kubernetes/ssl/ca.pem
          resources:
            requests:
              cpu: 200m
          livenessProbe:
            httpGet:
              host: 127.0.0.1
              path: /healthz
              port: 10252
            initialDelaySeconds: 15
            timeoutSeconds: 15
          volumeMounts:
          - mountPath: /etc/kubernetes/ssl
            name: ssl-certs-kubernetes
            readOnly: true
          - mountPath: /etc/ssl/certs
            name: ssl-certs-host
            readOnly: true
        hostNetwork: true
        volumes:
        - hostPath:
            path: /etc/kubernetes/ssl
          name: ssl-certs-kubernetes
        - hostPath:
            path: /usr/share/ca-certificates
          name: ssl-certs-host

  - path: /etc/kubernetes/manifests/kube-scheduler.yaml
    content: |
      apiVersion: v1
      kind: Pod
      metadata:
        name: kube-scheduler
        namespace: kube-system
      spec:
        hostNetwork: true
        containers:
        - name: kube-scheduler
          image: {{.HyperkubeImageRepo}}:{{.K8sVer}}
          command:
          - /hyperkube
          - scheduler
          - --master=http://127.0.0.1:8080
          - --leader-elect=true
          resources:
            requests:
              cpu: 100m
          livenessProbe:
            httpGet:
              host: 127.0.0.1
              path: /healthz
              port: 10251
            initialDelaySeconds: 15
            timeoutSeconds: 15

  - path: /srv/kubernetes/manifests/kubelet.yaml
    content: |
      apiVersion: extensions/v1beta1
      kind: DaemonSet
      metadata:
        name: kubelet
        namespace: kube-system
        labels:
          k8s-app: kubelet
          version: {{.K8sVer}}
      spec:
        template:
          metadata:
            labels:
              k8s-app: kubelet
              version: {{.K8sVer}}
          spec:
            containers:
            - name: kubelet
              image: {{.HyperkubeImageRepo}}:{{.K8sVer}}
              command:
              - /nsenter
              - --target=1
              - --mount
              - --wd=.
              - --
              - ./hyperkube
              - kubelet
              - --cni-conf-dir=/etc/kubernetes/cni/net.d
              - --network-plugin={{.K8sNetworkPlugin}}
              - --lock-file=/var/run/lock/kubelet.lock
              - --register-node
              - --allow-privileged
              - --pod-manifest-path=/etc/kubernetes/manifests
              - --cluster_dns={{.DNSServiceIP}}
              - --cluster_domain=cluster.local
              - --kubeconfig=/etc/kubernetes/kubeconfig
              - --require-kubeconfig
              - --tls-cert-file=/etc/kubernetes/ssl/worker.pem
              - --tls-private-key-file=/etc/kubernetes/ssl/worker-key.pem
              securityContext:
                privileged: true
              volumeMounts:
              - name: dev
                mountPath: /dev
              - name: run
                mountPath: /run
              - name: sys
                mountPath: /sys
                readOnly: true
              - name: etc-kubernetes
                mountPath: /etc/kubernetes
                readOnly: true
              - name: etc-ssl-certs
                mountPath: /etc/ssl/certs
                readOnly: true
              - name: var-lib-docker
                mountPath: /var/lib/docker
              - name: var-lib-kubelet
                mountPath: /var/lib/kubelet
              - name: var-lib-rkt
                mountPath: /var/lib/rkt
            hostNetwork: true
            hostPID: true
            volumes:
            - name: dev
              hostPath:
                path: /dev
            - name: run
              hostPath:
                path: /run
            - name: sys
              hostPath:
                path: /sys
            - name: etc-kubernetes
              hostPath:
                path: /etc/kubernetes
            - name: etc-ssl-certs
              hostPath:
                path: /usr/share/ca-certificates
            - name: var-lib-docker
              hostPath:
                path: /var/lib/docker
            - name: var-lib-kubelet
              hostPath:
                path: /var/lib/kubelet
            - name: var-lib-rkt
              hostPath:
                path: /var/lib/rkt

  - path: /srv/kubernetes/manifests/kube-proxy.yaml
    content: |
      apiVersion: "extensions/v1beta1"
      kind: DaemonSet
      metadata:
        name: kube-proxy
        namespace: kube-system
        labels:
          k8s_app: kube-proxy
          version: {{.K8sVer}}
      spec:
        template:
          metadata:
            labels:
              k8s_app: kube-proxy
              version: {{.K8sVer}}
          spec:
            hostNetwork: true
            containers:
            - name: kube-proxy
              image: {{.HyperkubeImageRepo}}:{{.K8sVer}}
              command:
              - /hyperkube
              - proxy
              - --kubeconfig=/etc/kubernetes/kubeconfig
              securityContext:
                privileged: true
              volumeMounts:
              - mountPath: /etc/ssl/certs
                name: ssl-certs-host
                readOnly: true
              - name: etc-kubernetes
                mountPath: /etc/kubernetes
                readOnly: true
            volumes:
            - hostPath:
                path: /usr/share/ca-certificates
              name: ssl-certs-host
            - name: etc-kubernetes
              hostPath:
                path: /etc/kubernetes

  - path: /srv/kubernetes/manifests/kube-dns-rc.yaml
    content: |
        apiVersion: v1
        kind: ReplicationController
        metadata:
          name: kube-dns-v19
          namespace: kube-system
          labels:
            k8s-app: kube-dns
            version: v19
            kubernetes.io/cluster-service: "true"
        spec:
          replicas: 1
          selector:
            k8s-app: kube-dns
            version: v19
          template:
            metadata:
              labels:
                k8s-app: kube-dns
                version: v19
                kubernetes.io/cluster-service: "true"
              annotations:
                scheduler.alpha.kubernetes.io/critical-pod: ''
                scheduler.alpha.kubernetes.io/tolerations: '[{"key":"CriticalAddonsOnly", "operator":"Exists"}]'
            spec:
              containers:
              - name: kubedns
                image: gcr.io/google_containers/kubedns-amd64:1.7
                resources:
                  limits:
                    memory: 170Mi
                  requests:
                    cpu: 100m
                    memory: 70Mi
                livenessProbe:
                  httpGet:
                    path: /healthz
                    port: 8080
                    scheme: HTTP
                  initialDelaySeconds: 60
                  timeoutSeconds: 5
                  successThreshold: 1
                  failureThreshold: 5
                readinessProbe:
                  httpGet:
                    path: /readiness
                    port: 8081
                    scheme: HTTP
                  initialDelaySeconds: 30
                  timeoutSeconds: 5
                args:
                - --domain=cluster.local.
                - --dns-port=10053
                ports:
                - containerPort: 10053
                  name: dns-local
                  protocol: UDP
                - containerPort: 10053
                  name: dns-tcp-local
                  protocol: TCP
              - name: dnsmasq
                image: gcr.io/google_containers/kube-dnsmasq-amd64:1.3
                args:
                - --cache-size=1000
                - --no-resolv
                - --server=127.0.0.1#10053
                - --log-facility=-
                ports:
                - containerPort: 53
                  name: dns
                  protocol: UDP
                - containerPort: 53
                  name: dns-tcp
                  protocol: TCP
              - name: healthz
                image: gcr.io/google_containers/exechealthz-amd64:1.1
                resources:
                  limits:
                    memory: 50Mi
                  requests:
                    cpu: 10m
                    memory: 50Mi
                args:
                - -cmd=nslookup kubernetes.default.svc.cluster.local 127.0.0.1 >/dev/null && nslookup kubernetes.default.svc.cluster.local 127.0.0.1:10053 >/dev/null
                - -port=8080
                - -quiet
                ports:
                - containerPort: 8080
                  protocol: TCP
              dnsPolicy: Default

  - path: /srv/kubernetes/manifests/kube-dns-svc.yaml
    content: |
        apiVersion: v1
        kind: Service
        metadata:
          name: kube-dns
          namespace: kube-system
          labels:
            k8s-app: kube-dns
            kubernetes.io/cluster-service: "true"
            kubernetes.io/name: "KubeDNS"
        spec:
          selector:
            k8s-app: kube-dns
          clusterIP: {{.DNSServiceIP}}
          ports:
          - name: dns
            port: 53
            protocol: UDP
          - name: dns-tcp
            port: 53
            protocol: TCP

  - path: /srv/kubernetes/manifests/heapster-de.yaml
    content: |
        apiVersion: extensions/v1beta1
        kind: Deployment
        metadata:
          name: heapster-v1.2.0
          namespace: kube-system
          labels:
            k8s-app: heapster
            kubernetes.io/cluster-service: "true"
            version: v1.2.0
        spec:
          replicas: 1
          selector:
            matchLabels:
              k8s-app: heapster
              version: v1.2.0
          template:
            metadata:
              labels:
                k8s-app: heapster
                version: v1.2.0
              annotations:
                scheduler.alpha.kubernetes.io/critical-pod: ''
                scheduler.alpha.kubernetes.io/tolerations: '[{"key":"CriticalAddonsOnly", "operator":"Exists"}]'
            spec:
              containers:
                - image: gcr.io/google_containers/heapster:v1.2.0
                  name: heapster
                  livenessProbe:
                    httpGet:
                      path: /healthz
                      port: 8082
                      scheme: HTTP
                    initialDelaySeconds: 180
                    timeoutSeconds: 5
                  resources:
                    limits:
                      cpu: 80m
                      memory: 200Mi
                    requests:
                      cpu: 80m
                      memory: 200Mi
                  command:
                    - /heapster
                    - --source=kubernetes.summary_api:''
                - image: gcr.io/google_containers/addon-resizer:1.6
                  name: heapster-nanny
                  resources:
                    limits:
                      cpu: 50m
                      memory: 90Mi
                    requests:
                      cpu: 50m
                      memory: 90Mi
                  env:
                    - name: MY_POD_NAME
                      valueFrom:
                        fieldRef:
                          fieldPath: metadata.name
                    - name: MY_POD_NAMESPACE
                      valueFrom:
                        fieldRef:
                          fieldPath: metadata.namespace
                  command:
                    - /pod_nanny
                    - --cpu=80m
                    - --extra-cpu=4m
                    - --memory=200Mi
                    - --extra-memory=4Mi
                    - --threshold=5
                    - --deployment=heapster-v1.2.0
                    - --container=heapster
                    - --poll-period=300000
                    - --estimator=exponential

  - path: /srv/kubernetes/manifests/heapster-svc.yaml
    content: |
        kind: Service
        apiVersion: v1
        metadata:
          name: heapster
          namespace: kube-system
          labels:
            kubernetes.io/cluster-service: "true"
            kubernetes.io/name: "Heapster"
        spec:
          ports:
            - port: 80
              targetPort: 8082
          selector:
            k8s-app: heapster

  - path: /srv/kubernetes/manifests/kube-dashboard-rc.yaml
    content: |
        apiVersion: v1
        kind: ReplicationController
        metadata:
          name: kubernetes-dashboard-v1.4.0
          namespace: kube-system
          labels:
            k8s-app: kubernetes-dashboard
            version: v1.4.0
            kubernetes.io/cluster-service: "true"
        spec:
          replicas: 1
          selector:
            k8s-app: kubernetes-dashboard
          template:
            metadata:
              labels:
                k8s-app: kubernetes-dashboard
                version: v1.4.0
                kubernetes.io/cluster-service: "true"
              annotations:
                scheduler.alpha.kubernetes.io/critical-pod: ''
                scheduler.alpha.kubernetes.io/tolerations: '[{"key":"CriticalAddonsOnly", "operator":"Exists"}]'
            spec:
              containers:
              - name: kubernetes-dashboard
                image: gcr.io/google_containers/kubernetes-dashboard-amd64:v1.4.0
                resources:
                  limits:
                    cpu: 100m
                    memory: 50Mi
                  requests:
                    cpu: 100m
                    memory: 50Mi
                ports:
                - containerPort: 9090
                livenessProbe:
                  httpGet:
                    path: /
                    port: 9090
                  initialDelaySeconds: 30
                  timeoutSeconds: 30

  - path: /srv/kubernetes/manifests/kube-dashboard-svc.yaml
    content: |
        apiVersion: v1
        kind: Service
        metadata:
          name: kubernetes-dashboard
          namespace: kube-system
          labels:
            k8s-app: kubernetes-dashboard
            kubernetes.io/cluster-service: "true"
        spec:
          selector:
            k8s-app: kubernetes-dashboard
          ports:
          - port: 80
            targetPort: 9090

  - path: /etc/kubernetes/ssl/ca.pem.enc
    # encoding: gzip+base64
    encoding: base64
    content: {{.TLSConfig.CACert}}

  - path: /etc/kubernetes/ssl/apiserver.pem.enc
    # encoding: gzip+base64
    encoding: base64
    content: {{.TLSConfig.APIServerCert}}

  - path: /etc/kubernetes/ssl/apiserver-key.pem.enc
    # encoding: gzip+base64
    encoding: base64
    content: {{.TLSConfig.APIServerKey}}

  - path: /etc/kubernetes/ssl/worker.pem.enc
    # encoding: gzip+base64
    encoding: base64
    content: {{.TLSConfig.WorkerCert}}

  - path: /etc/kubernetes/ssl/worker-key.pem.enc
    # encoding: gzip+base64
    encoding: base64
    content: {{.TLSConfig.WorkerKey}}

  - path: /etc/kubernetes/cni/net.d/10-flannel.conf
    content: |
        {
            "name": "podnet",
            "type": "flannel",
            "delegate": {
                "isDefaultGateway": true
            }
        }
